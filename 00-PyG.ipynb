{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcxOYKbFD0IL7zhEU22mXI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/GPT-Graph/blob/main/00-PyG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### https://pub.towardsai.net/graph-neural-networks-for-fraud-detection-in-crypto-transactions-6b252325633f\n",
        "\n",
        "### https://mariazork.github.io/"
      ],
      "metadata": {
        "id": "qbROaSj9I52u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnhcbr60Inrt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Dropout\n",
        "from torch_geometric.nn import GCNConv, GATv2Conv\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    seed = 0\n",
        "    learning_rate = 0.001\n",
        "    weight_decay = 1e-5\n",
        "    input_dim = 165\n",
        "    output_dim = 1\n",
        "    hidden_size = 128\n",
        "    num_epochs = 100\n",
        "    checkpoints_dir = './models/elliptic_gnn'\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"Using device:\", Config.device)"
      ],
      "metadata": {
        "id": "vLaW_fH3I01v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import EllipticBitcoinDataset\n",
        "dataset = EllipticBitcoinDataset(root=’./data/elliptic-bitcoin-dataset’)"
      ],
      "metadata": {
        "id": "TgSo9obwIqNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_features = pd.read_csv('./data/elliptic_bitcoin_dataset/elliptic_txs_features.csv', header=None)\n",
        "df_edges = pd.read_csv(\"./data/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv\")\n",
        "df_classes =  pd.read_csv(\"./data/elliptic_bitcoin_dataset/elliptic_txs_classes.csv\")\n",
        "df_classes['class'] = df_classes['class'].map({'unknown': 2, '1': 1, '2': 0})"
      ],
      "metadata": {
        "id": "zUsaQVNFIzBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here column 0 stands for node_id, column 1 is the time axis\n",
        "df_features.head()"
      ],
      "metadata": {
        "id": "aFnMHW0hI4Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_edges.head()"
      ],
      "metadata": {
        "id": "pZ3r_3OQJB8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_classes.head()"
      ],
      "metadata": {
        "id": "ojWJLJXxJDw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_classes['class'].value_counts()"
      ],
      "metadata": {
        "id": "cmBS8j_jJGbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merging node features DF with classes DF\n",
        "df_merge = df_features.merge(df_classes, how='left', right_on=\"txId\", left_on=0)\n",
        "df_merge = df_merge.sort_values(0).reset_index(drop=True)\n",
        "\n",
        "# extracting classified/non-classified nodes\n",
        "classified = df_merge.loc[df_merge['class'].loc[df_merge['class']!=2].index].drop('txId', axis=1)\n",
        "unclassified = df_merge.loc[df_merge['class'].loc[df_merge['class']==2].index].drop('txId', axis=1)\n",
        "\n",
        "# extracting classified/non-classified edges\n",
        "classified_edges = df_edges.loc[df_edges['txId1'].isin(classified[0]) & df_edges['txId2'].isin(classified[0])]\n",
        "unclassifed_edges = df_edges.loc[df_edges['txId1'].isin(unclassified[0]) | df_edges['txId2'].isin(unclassified[0])]"
      ],
      "metadata": {
        "id": "0qXn3-PVJI_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping nodes to indices\n",
        "nodes = df_merge[0].values\n",
        "map_id = {j:i for i,j in enumerate(nodes)}\n",
        "\n",
        "# mapping edges to indices\n",
        "edges = df_edges.copy()\n",
        "edges.txId1 = edges.txId1.map(map_id)\n",
        "edges.txId2 = edges.txId2.map(map_id)\n",
        "edges = edges.astype(int)\n",
        "\n",
        "edge_index = np.array(edges.values).T\n",
        "edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n",
        "\n",
        "# weights for the edges are equal in case of model without attention\n",
        "weights = torch.tensor([1] * edge_index.shape[1] , dtype=torch.float32)\n",
        "\n",
        "print(\"Total amount of edges in DAG:\", edge_index.shape)"
      ],
      "metadata": {
        "id": "oolH2SO3JNck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# maping node ids to corresponding indexes\n",
        "node_features = df_merge.drop(['txId'], axis=1).copy()\n",
        "node_features[0] = node_features[0].map(map_id)\n",
        "\n",
        "classified_idx = node_features['class'].loc[node_features['class']!=2].index\n",
        "unclassified_idx = node_features['class'].loc[node_features['class']==2].index\n",
        "\n",
        "# replace unkown class with 0, to avoid having 3 classes, this data/labels never used in training\n",
        "node_features['class'] = node_features['class'].replace(2, 0)\n",
        "\n",
        "labels = node_features['class'].values\n",
        "\n",
        "# drop indeces, class and temporal axes\n",
        "node_features = torch.tensor(np.array(node_features.drop([0, 'class', 1], axis=1).values, dtype=np.float32), dtype=torch.float32)"
      ],
      "metadata": {
        "id": "wdE1ECcOJQqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting data to PyGeometric graph data format\n",
        "elliptic_dataset = Data(x = node_features,\n",
        "                        edge_index = edge_index,\n",
        "                        edge_attr = weights,\n",
        "                        y = torch.tensor(labels, dtype=torch.float32))\n",
        "\n",
        "print(f'Number of nodes: {elliptic_dataset.num_nodes}')\n",
        "print(f'Number of node features: {elliptic_dataset.num_features}')\n",
        "print(f'Number of edges: {elliptic_dataset.num_edges}')\n",
        "print(f'Number of edge features: {elliptic_dataset.num_features}')\n",
        "print(f'Average node degree: {elliptic_dataset.num_edges / elliptic_dataset.num_nodes:.2f}')\n",
        "print(f'Number of classes: {len(np.unique(elliptic_dataset.y))}')\n",
        "print(f'Has isolated nodes: {elliptic_dataset.has_isolated_nodes()}')\n",
        "print(f'Has self loops: {elliptic_dataset.has_self_loops()}')\n",
        "print(f'Is directed: {elliptic_dataset.is_directed()}')"
      ],
      "metadata": {
        "id": "_2QV8WyoJUbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = labels[classified_idx]\n",
        "\n",
        "# spliting train set and validation set\n",
        "_, _, _, _, train_idx, valid_idx = \\\n",
        "    train_test_split(node_features[classified_idx],\n",
        "                     y_train,\n",
        "                     classified_idx,\n",
        "                     test_size=0.15,\n",
        "                     random_state=Config.seed,\n",
        "                     stratify=y_train)\n",
        "\n",
        "elliptic_dataset.train_idx = torch.tensor(train_idx, dtype=torch.long)\n",
        "elliptic_dataset.val_idx = torch.tensor(valid_idx, dtype=torch.long)\n",
        "elliptic_dataset.test_idx = torch.tensor(unclassified_idx, dtype=torch.long)\n",
        "\n",
        "print(\"Train dataset size:\", elliptic_dataset.train_idx.shape[0])\n",
        "print(\"Validation dataset size:\", elliptic_dataset.val_idx.shape[0])\n",
        "print(\"Test dataset size:\", elliptic_dataset.test_idx.shape[0])"
      ],
      "metadata": {
        "id": "EJyp9h9JJZAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    \"\"\"Graph Convolutional Network\"\"\"\n",
        "    def __init__(self, dim_in, dim_h, dim_out):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gcn1 = GCNConv(dim_in, dim_h)\n",
        "        self.gcn2 = GCNConv(dim_h, dim_out)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        h = self.gcn1(x, edge_index)\n",
        "        h = torch.relu(h)\n",
        "        h = F.dropout(h, p=0.6, training=self.training)\n",
        "        out = self.gcn2(h, edge_index)\n",
        "        return out\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    \"\"\"Graph Attention Network\"\"\"\n",
        "    def __init__(self, dim_in, dim_h, dim_out, heads=8):\n",
        "        super(GAT, self).__init__()\n",
        "        self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads, dropout=0.6)\n",
        "        self.gat2 = GATv2Conv(dim_h*heads, dim_out, concat=False, heads=1, dropout=0.6)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        h = F.dropout(x, p=0.6, training=self.training)\n",
        "        h = self.gat1(h, edge_index)\n",
        "        h = F.elu(h)\n",
        "        h = F.dropout(h, p=0.6, training=self.training)\n",
        "        out = self.gat2(h, edge_index)\n",
        "        return out\n",
        "\n",
        "def accuracy(y_pred, y_test, prediction_threshold=0.5):\n",
        "    y_pred_label = (torch.sigmoid(y_pred) > prediction_threshold).float()*1\n",
        "\n",
        "    correct_results_sum = (y_pred_label == y_test).sum().float()\n",
        "    acc = correct_results_sum/y_test.shape[0]\n",
        "\n",
        "    return acc"
      ],
      "metadata": {
        "id": "3IUlBqj3JfPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate(model, data, criterion, optimizer, *args):\n",
        "    num_epochs = args[0]\n",
        "    checkpoints_dir = args[1]\n",
        "    model_filename = args[2]\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    best_loss = 10e10\n",
        "\n",
        "    if not os.path.exists(checkpoints_dir):\n",
        "        os.makedirs(checkpoints_dir)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs+1):\n",
        "        # Training\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)\n",
        "        loss = criterion(out[data.train_idx], data.y[data.train_idx].unsqueeze(1))\n",
        "        acc = accuracy(out[data.train_idx], data.y[data.train_idx].unsqueeze(1), prediction_threshold=0.5)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        val_loss = criterion(out[data.val_idx], data.y[data.val_idx].unsqueeze(1))\n",
        "        val_acc = accuracy(out[data.val_idx], data.y[data.val_idx].unsqueeze(1), prediction_threshold=0.5)\n",
        "\n",
        "        if(epoch % 10 == 0):\n",
        "            print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: '\n",
        "                  f'{acc*100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n",
        "                  f'Val Acc: {val_acc*100:.2f}%')\n",
        "\n",
        "            if val_loss < best_loss:\n",
        "                best_loss = val_loss\n",
        "                print(\"Saving model for best loss\")\n",
        "                checkpoint = {\n",
        "                    'state_dict': best_model_wts\n",
        "                }\n",
        "                torch.save(checkpoint, os.path.join(checkpoints_dir, model_filename))\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    return model\n",
        "\n",
        "def test(model, data):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    preds = ((torch.sigmoid(out) > 0.5).float()*1).squeeze(1)\n",
        "    return preds"
      ],
      "metadata": {
        "id": "-aGWNHwOJgV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcn_model = GCN(Config.input_dim, Config.hidden_size, Config.output_dim).to(Config.device)\n",
        "data_train = elliptic_dataset.to(Config.device)\n",
        "\n",
        "optimizer = torch.optim.Adam(gcn_model.parameters(), lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "train_evaluate(gcn_model,\n",
        "              data_train,\n",
        "              criterion,\n",
        "              optimizer,\n",
        "              Config.num_epochs,\n",
        "              Config.checkpoints_dir,\n",
        "              'gcn_best_model.pth.tar')"
      ],
      "metadata": {
        "id": "fuKaQgg-Jkr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcn_model.load_state_dict(torch.load(os.path.join(Config.checkpoints_dir, 'gcn_best_model.pth.tar'))['state_dict'])\n",
        "\n",
        "y_test_preds = test(gcn_model, data_train)\n",
        "\n",
        "# confusion matrix on validation data\n",
        "conf_mat = confusion_matrix(data_train.y[data_train.val_idx].detach().cpu().numpy(), y_test_preds[valid_idx])\n",
        "\n",
        "plt.subplots(figsize=(6,6))\n",
        "sns.set(font_scale=1.4)\n",
        "sns.heatmap(conf_mat, annot=True, fmt=\".0f\", annot_kws={\"size\": 16}, cbar=False)\n",
        "plt.xlabel('Target (true) Class'); plt.ylabel('Output (predicted) class'); plt.title('Confusion Matrix')\n",
        "plt.show();\n",
        "\n",
        "print(classification_report(data_train.y[data_train.val_idx].detach().cpu().numpy(),\n",
        "                            y_test_preds[valid_idx],\n",
        "                            target_names=['licit', 'illicit']))\n",
        "\n",
        "\n",
        "print(f\"Test data fraud cases, percentage: {y_test_preds[data_train.test_idx].detach().cpu().numpy().sum() / len(data_train.y[data_train.test_idx]) *100} %\")"
      ],
      "metadata": {
        "id": "ih_hGIHoJpAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gat_model = GAT(Config.input_dim, Config.hidden_size, Config.output_dim).to(Config.device)\n",
        "data_train = elliptic_dataset.to(Config.device)\n",
        "\n",
        "optimizer = torch.optim.Adam(gat_model.parameters(), lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "train_evaluate(gat_model,\n",
        "               data_train,\n",
        "               criterion,\n",
        "               optimizer,\n",
        "               Config.num_epochs,\n",
        "               Config.checkpoints_dir,\n",
        "               'gat_best_model.pth.tar')"
      ],
      "metadata": {
        "id": "E05WsrJgJs66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gat_model.load_state_dict(torch.load(os.path.join(Config.checkpoints_dir, 'gat_best_model.pth.tar'))['state_dict'])\n",
        "\n",
        "y_test_preds = test(gat_model, data_train)\n",
        "\n",
        "# confusion matrix on validation data\n",
        "conf_mat = confusion_matrix(data_train.y[data_train.val_idx].detach().cpu().numpy(), y_test_preds[valid_idx])\n",
        "\n",
        "plt.subplots(figsize=(6,6))\n",
        "sns.set(font_scale=1.4)\n",
        "sns.heatmap(conf_mat, annot=True, fmt=\".0f\", annot_kws={\"size\": 16}, cbar=False)\n",
        "plt.xlabel('Target (true) Class'); plt.ylabel('Output (predicted) class'); plt.title('Confusion Matrix')\n",
        "plt.show();\n",
        "\n",
        "print(classification_report(data_train.y[data_train.val_idx].detach().cpu().numpy(),\n",
        "                            y_test_preds[valid_idx],\n",
        "                            target_names=['licit', 'illicit']))\n",
        "\n",
        "\n",
        "print(f\"Test data fraud cases, percentage: {y_test_preds[data_train.test_idx].detach().cpu().numpy().sum() / len(data_train.y[data_train.test_idx]) *100} %\")\n"
      ],
      "metadata": {
        "id": "iq7BVN2pJv-0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}